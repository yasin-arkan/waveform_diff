{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasin-arkan/waveform_diff/blob/main/diffusion_flow_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GswAUDOqEpLl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpntowngal47"
      },
      "outputs": [],
      "source": [
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tNm9H2SeHlD"
      },
      "outputs": [],
      "source": [
        "%cd gdrive/MyDrive/diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTN1LJfwnNgx"
      },
      "source": [
        "## Preparing the data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twA8nVh7W_RV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from geopy import distance\n",
        "\n",
        "import librosa\n",
        "\n",
        "\n",
        "N_FFT = 256\n",
        "HOP_LENGTH = 64\n",
        "WIN_LENGTH = N_FFT\n",
        "\n",
        "file_path = \"data/timeseries_EW.csv\"\n",
        "\n",
        "def load_data(path, n_fft, hop_length, win_length, batch_size=16):\n",
        "\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    cond_cols = ['Depth', 'Magnitude']\n",
        "    cond_vars = data[cond_cols].copy()\n",
        "\n",
        "    cond_data = []\n",
        "    norm_dict = {}\n",
        "\n",
        "    dist = source_site_distance(data)\n",
        "    cond_vars.loc[:, ('Distance')] = dist\n",
        "    cond_cols.append('Distance')\n",
        "\n",
        "    angle = compute_angle(data)\n",
        "    cond_vars.loc[:, ('Angle')] = angle\n",
        "    cond_cols.append('Angle')\n",
        "\n",
        "    for cvar in cond_cols:\n",
        "        cv = cond_vars[cvar].to_numpy()\n",
        "        cv = cv.reshape(cv.shape[0], 1)\n",
        "\n",
        "        cv_mean, cv_std = cv.mean(), cv.std()\n",
        "        norm_dict[cvar] = [cv_mean, cv_std]\n",
        "\n",
        "        cv = (cv - cv_mean) / cv_std\n",
        "\n",
        "        cond_data.append(cv)\n",
        "\n",
        "\n",
        "    wfs = data.iloc[:, 16:].to_numpy()\n",
        "\n",
        "    orig_wfs = wfs.copy()\n",
        "\n",
        "    wfs = librosa.stft(wfs, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
        "\n",
        "    wfs = np.abs(wfs)\n",
        "\n",
        "    print(wfs.shape)\n",
        "    wfs = torch.from_numpy(wfs).float()  # [1183, 128, 110]\n",
        "\n",
        "    print(\"Before padding:\", wfs.shape)\n",
        "    current_time_dim = wfs.shape[2] # Should be 110, we will pad it to 128\n",
        "    padding_needed = 128 - current_time_dim\n",
        "    time_padding = (0, padding_needed)\n",
        "    wfs = F.pad(wfs, time_padding, mode='constant', value=0)\n",
        "\n",
        "    wfs = wfs[:, :64, :]\n",
        "    print(\"After padding:\" ,wfs.shape) # Now it is [1183, 64, 128]\n",
        "\n",
        "\n",
        "    # We get the length for now and reshape the wfs to squeeze last 2 dimensions,\n",
        "    # so we can normalize them\n",
        "    length, x, y = wfs.shape\n",
        "\n",
        "    wfs = wfs.reshape((length, -1)) # wfs = [1183, 8192]\n",
        "\n",
        "    wfs_mean, wfs_std = wfs.mean(), wfs.std()\n",
        "    wfs = (wfs - wfs_mean) / wfs_std\n",
        "\n",
        "    wfs = wfs.reshape((length, x, y))\n",
        "\n",
        "    cond_var = np.concatenate(cond_data, axis=1)\n",
        "    cond_var = torch.from_numpy(cond_var)\n",
        "\n",
        "\n",
        "    train_dataset = STFTDataset(wfs[:1120, :, :], cond_var[:1120, :])\n",
        "    val_dataset = STFTDataset(wfs[1120:, :, :], cond_var[1120:, :])\n",
        "    all_dataset = STFTDataset(wfs, cond_var)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    all_dataloader = DataLoader(all_dataset,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(\"Waveform shape:\", train_dataset.wfs.shape)\n",
        "    print(\"Cond shape:\", train_dataset.cond_var.shape)\n",
        "\n",
        "    print(\"Waveform shape (val):\", val_dataset.wfs.shape)\n",
        "    print(\"Cond shape (val):\", val_dataset.cond_var.shape)\n",
        "\n",
        "    return train_dataset, train_dataloader, val_dataset, val_dataloader, all_dataset, all_dataloader, wfs_mean, wfs_std, norm_dict, orig_wfs, cond_vars\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class STFTDataset(Dataset):\n",
        "    def __init__(self, wfs, cond_var):\n",
        "        # Ensure data is already scaled and in tensor format\n",
        "        self.wfs = wfs\n",
        "        self.cond_var = cond_var\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.wfs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wfs[idx], self.cond_var[idx]\n",
        "\n",
        "\n",
        "\n",
        "def source_site_distance(info):\n",
        "    '''compute the source-site distance'''\n",
        "    # reference: https://geopy.readthedocs.io/en/stable/#module-geopy.distance\n",
        "    # src_loc: (lat, long)\n",
        "\n",
        "    # source location: (lat, long)\n",
        "    src_lat = info.EventLat.to_numpy() # [#sample,]\n",
        "    src_lon = info.EventLon.to_numpy()\n",
        "\n",
        "    # station location: (lat, long)\n",
        "    station_lat = info.StationLat.to_numpy()\n",
        "    station_lon = info.StationLon.to_numpy()\n",
        "\n",
        "    # calculate the source-site distance\n",
        "    dist = []\n",
        "    for i in range(len(src_lat)):\n",
        "        dist_val = distance.distance((src_lat[i], src_lon[i]), (station_lat[i], station_lon[i])).km\n",
        "        dist.append(dist_val)\n",
        "\n",
        "    dist = np.array(dist).reshape(len(dist),1)\n",
        "\n",
        "    return dist\n",
        "\n",
        "\n",
        "\n",
        "def compute_angle(info):\n",
        "\n",
        "    '''\n",
        "    Calculate the angle between source centers and station locations\n",
        "    '''\n",
        "\n",
        "    # source location: (lat, long)\n",
        "    src_lat = info['EventLat'].to_numpy() # [#samples,]\n",
        "    src_lon = info['EventLon'].to_numpy()\n",
        "\n",
        "    # station location: (lat, long)\n",
        "    station_lat = info['StationLat'].to_numpy()\n",
        "    station_lon = info['StationLon'].to_numpy()\n",
        "\n",
        "    # calculate the source-site angle\n",
        "    angle = []\n",
        "    for i in range(len(src_lat)):\n",
        "        src_coord = (src_lat[i], src_lon[i])\n",
        "        station_coord = (station_lat[i], station_lon[i])\n",
        "\n",
        "        # Calculate the vector from the source center to the station location\n",
        "        vector = np.array(station_coord) - np.array(src_coord)\n",
        "\n",
        "        # Calculate the angle between the vectors and the x-axis (east direction)\n",
        "        angle_rad = np.arctan2(vector[1], vector[0])  # Angle in radians\n",
        "\n",
        "        # Convert the angle to degrees\n",
        "        angle_deg = np.degrees(angle_rad)\n",
        "\n",
        "        angle.append(angle_deg)\n",
        "\n",
        "    angle = np.array(angle).reshape(len(angle),1)\n",
        "\n",
        "    return angle\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset, train_dataloader, val_dataset, val_dataloader, all_dataset, all_dataloader, wfs_mean, wfs_std, norm_dict, orig_wfs, cond_vars = load_data(file_path,\n",
        "                                                                                                                                                          N_FFT,\n",
        "                                                                                                                                                          HOP_LENGTH,\n",
        "                                                                                                                                                          WIN_LENGTH,\n",
        "                                                                                                                                                          batch_size=16)\n",
        "\n",
        "\n",
        "def plot_sample_stft(index):\n",
        "\n",
        "  spectrogram = train_dataset.wfs[index].cpu().numpy()\n",
        "\n",
        "  mean = wfs_mean.numpy()\n",
        "  std = wfs_std.numpy()\n",
        "\n",
        "  spectrogram = (spectrogram * std) + mean\n",
        "\n",
        "  tf = librosa.griffinlim(spectrogram, n_iter=512)\n",
        "\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "\n",
        "  ax1.set_title(f\"Waveform for sample {index}\")\n",
        "  ax1.set_xlabel(\"Time (seconds)\")\n",
        "  ax1.set_ylabel(\"Amplitude\")\n",
        "  ax1.plot(tf)\n",
        "\n",
        "\n",
        "  print(spectrogram.shape)\n",
        "  ax2.imshow(spectrogram, aspect='auto')\n",
        "  ax2.set_xlabel('Time Frame')\n",
        "  ax2.set_ylabel('Frequency Bin')\n",
        "  ax2.set_title(f\"Spectrogram for Sample {index}\")\n",
        "\n",
        "\n",
        "  ax3.set_title(\"Original waveform\")\n",
        "  orig = orig_wfs[index]\n",
        "  ax3.plot(orig)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# plot_sample_stft(333)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install flow_matching"
      ],
      "metadata": {
        "id": "BMZN-GmBA7mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flow_matching\n",
        "from flow_matching.path.scheduler import CondOTScheduler\n",
        "from flow_matching.path import AffineProbPath\n",
        "from flow_matching.solver import Solver, ODESolver\n",
        "from flow_matching.utils import ModelWrapper\n",
        "\n",
        "path = AffineProbPath(scheduler=CondOTScheduler())\n"
      ],
      "metadata": {
        "id": "SWRZW-zsA1y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv2RqizYnYCp"
      },
      "source": [
        "## Config, model and training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZmDQr2gSNBZ"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel, UNet2DConditionModel\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet2DConditionModel(\n",
        "    sample_size=(64, 128),\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    layers_per_block=2, # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"CrossAttnDownBlock2D\", # Use CrossAttn blocks where you want to inject conditioning\n",
        "        \"CrossAttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"CrossAttnUpBlock2D\",\n",
        "        \"CrossAttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        "\n",
        "    cross_attention_dim=512,\n",
        ")\n",
        "\n",
        "\n",
        "cond_emb = nn.Sequential(\n",
        "    nn.Linear(4, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 512)\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WrappedModel(ModelWrapper):\n",
        "\n",
        "    def __init__(self, model, num_train_timesteps):\n",
        "        super().__init__(model)\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor, h: torch.Tensor):\n",
        "        model_input_t = (t * (self.num_train_timesteps - 1)).long()\n",
        "        return self.model(x, model_input_t, h).sample"
      ],
      "metadata": {
        "id": "hx1lYbqZXiQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi0Bo1fRi1Qk"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline, UNet2DConditionModel\n",
        "\n",
        "class CustomDDPMPipeline(DDPMPipeline):\n",
        "    def __init__(self, unet, scheduler):\n",
        "        super().__init__(unet, scheduler)\n",
        "\n",
        "    def __call__(self, batch_size=1, generator=None, num_inference_steps=None, output_type=\"pil\", return_dict=True, encoder_hidden_states=None):\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        image = torch.randn(\n",
        "            (batch_size, self.unet.config.in_channels, self.unet.config.sample_size[0], self.unet.config.sample_size[1]),\n",
        "            generator=generator,\n",
        "            )\n",
        "\n",
        "        image = image.to(self.device)\n",
        "        wrapped_unet = WrappedModel(self.unet, self.scheduler.config.num_train_timesteps)\n",
        "\n",
        "        solver = ODESolver(velocity_model=wrapped_unet)\n",
        "\n",
        "        T = torch.linspace(0, 1, 10)\n",
        "        T = T.to(self.device)\n",
        "\n",
        "        # # set step values\n",
        "        # if num_inference_steps is None:\n",
        "        #     num_inference_steps = self.scheduler.config.num_train_timesteps\n",
        "        # timesteps = self.scheduler.timesteps[-num_inference_steps:]\n",
        "\n",
        "        # print(T.shape)\n",
        "        # print(image.shape)\n",
        "\n",
        "\n",
        "        # self.scheduler.set_timesteps(num_inference_steps)\n",
        "        # timesteps = self.scheduler.timesteps\n",
        "\n",
        "\n",
        "        solver_args = {'h': encoder_hidden_states}\n",
        "        intermediate_samples = solver.sample(time_grid=T, x_init=image, method='midpoint', step_size=0.05, return_intermediates=True, **solver_args)\n",
        "        image = intermediate_samples[-1]\n",
        "\n",
        "        # image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        if not return_dict:\n",
        "           return (image,), intermediate_samples, T\n",
        "\n",
        "        return (image,), intermediate_samples, T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-mw_fQYSf3s"
      },
      "outputs": [],
      "source": [
        "def evaluate(config, epoch, pipeline, hidden, original):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "\n",
        "    # Output of the CustomDDPMPipeline above\n",
        "    images, samples, T = pipeline(\n",
        "        batch_size = config.eval_batch_size,\n",
        "        generator=torch.manual_seed(config.seed),\n",
        "        output_type=\"numpy\",\n",
        "        return_dict=False,\n",
        "        encoder_hidden_states=hidden\n",
        "    )\n",
        "\n",
        "    # Preparing the folders\n",
        "    arrs_dir =  os.path.join(config.output_dir, \"arrays\")\n",
        "    images_dir = os.path.join(config.output_dir, \"images\")\n",
        "    os.makedirs(arrs_dir, exist_ok=True)\n",
        "    os.makedirs(images_dir, exist_ok=True)\n",
        "    eval_batch_size = config.eval_batch_size\n",
        "\n",
        "\n",
        "    img_array = images[0].cpu().squeeze()\n",
        "\n",
        "\n",
        "    # Saving the output as an array here\n",
        "    np.save(os.path.join(arrs_dir, f\"epoch_{epoch:04d}_generated_sample.npy\"), img_array)\n",
        "\n",
        "\n",
        "    # Plotting the original and the output of the pipeline for comparison\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    original = original.cpu().numpy().squeeze()\n",
        "\n",
        "    ax1.set_title(f\"Original Sample, Epoch {epoch}\")\n",
        "    ax1.set_xlabel('Time Frame')\n",
        "    ax1.set_ylabel('Frequency Bin')\n",
        "    ax1.imshow(original, aspect='auto')\n",
        "\n",
        "    ax2.set_title(f\"Generated Sample, Epoch {epoch}\")\n",
        "    ax2.set_xlabel('Time Frame')\n",
        "    ax2.set_ylabel('Frequency Bin')\n",
        "    ax2.imshow(img_array, aspect='auto')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(os.path.join(images_dir, f\"epoch_{epoch:04d}_orig_vs_gen.png\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    # # Plotting the probability path (Gaussian noise at t=0, final generated image at t=1) (Optional)\n",
        "    # fig, axs = plt.subplots(2, 5,figsize=(20,10))\n",
        "\n",
        "    # samples = samples.cpu()\n",
        "\n",
        "    # axs = axs.flatten()\n",
        "\n",
        "    # for i, s in enumerate(samples):\n",
        "    #   s = s.squeeze()\n",
        "    #   axs[i].imshow(s, aspect='auto')\n",
        "    #   axs[i].axis('off')\n",
        "    #   axs[i].set_title(f't= %.2f' % (T[i]))\n",
        "\n",
        "    # plt.tight_layout()\n",
        "\n",
        "    # plt.savefig(os.path.join(images_dir, f\"epoch_{epoch:04d}_path_timesteps.png\"))\n",
        "    # plt.close()\n",
        "\n",
        "    print(f\"Saved {eval_batch_size} sample images to {images_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3glAitL_U30A"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from accelerate.utils import ProjectConfiguration\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "from diffusers import DDPMPipeline\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, val_dataset, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    logging_dir = os.path.join(config.output_dir, \"logs\")\n",
        "    accelerator_project_config = ProjectConfiguration(project_dir=config.output_dir, logging_dir=logging_dir)\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_config=accelerator_project_config,\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.push_to_hub:\n",
        "            repo_id = create_repo(\n",
        "                repo_id=Path(config.output_dir).name, exist_ok=True\n",
        "            ).repo_id\n",
        "        elif config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, you just need to unpack the\n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    id = random.randint(0, len(val_dataset) - 1)\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            magnitudes, conds = batch\n",
        "\n",
        "            x_1 = magnitudes.to(accelerator.device).unsqueeze(1) # Add channel dimension\n",
        "            conds = conds.to(accelerator.device).float()\n",
        "            conds = cond_emb(conds)\n",
        "            conds = conds.unsqueeze(1)\n",
        "\n",
        "            # Sample noise to add to the images\n",
        "            x_0 = torch.randn(x_1.shape).to(x_1.device)\n",
        "            bs = x_1.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            t = torch.rand(x_1.shape[0]).to(x_1.device)\n",
        "            path_sample = path.sample(t=t, x_0=x_0, x_1=x_1)\n",
        "\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # # Predict the noise residual\n",
        "                model_input_t = (path_sample.t * (noise_scheduler.config.num_train_timesteps - 1)).long()\n",
        "\n",
        "                predicted_velocity = model(path_sample.x_t, model_input_t, conds).sample\n",
        "                loss = torch.pow(predicted_velocity - path_sample.dx_t, 2).mean()\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            with torch.no_grad():\n",
        "              pipeline = CustomDDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "              wfs_val = val_dataset.wfs\n",
        "              cond_val = val_dataset.cond_var\n",
        "\n",
        "              sample_wfs = wfs_val[id].to(accelerator.device)\n",
        "\n",
        "              sample_cond = cond_val[id].to(accelerator.device).float()\n",
        "              sample_cond = cond_emb(sample_cond)\n",
        "              sample_cond = sample_cond.unsqueeze(0)\n",
        "              sample_cond = sample_cond.unsqueeze(0)\n",
        "\n",
        "\n",
        "              if (epoch) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1 or epoch == 0:\n",
        "                  evaluate(config, epoch, pipeline, sample_cond, sample_wfs)\n",
        "\n",
        "              if (epoch) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                  pipeline.save_pretrained(config.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tzuwh8ZzNHW"
      },
      "outputs": [],
      "source": [
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from dataclasses import dataclass\n",
        "from diffusers import FlowMatchEulerDiscreteScheduler\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = (64, 128)  # the generated image resolution\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 1 # how many images to sample during evaluation\n",
        "    num_epochs = 120\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-5\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 5\n",
        "    save_model_epochs = 15\n",
        "    mixed_precision = 'no'  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = 'checkpoints_EW_flow_1e-5'  # the model namy locally and on the HF Hub\n",
        "    data_path = 'data/timeseries_EW.csv'\n",
        "\n",
        "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
        "    hub_private_repo = False\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 0\n",
        "\n",
        "config = TrainingConfig()\n",
        "\n",
        "\n",
        "noise_scheduler = FlowMatchEulerDiscreteScheduler(num_train_timesteps=1000)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3bVHv-uukib"
      },
      "outputs": [],
      "source": [
        "file_path = config.data_path\n",
        "\n",
        "train_dataset, train_dataloader, val_dataset, val_dataloader, all_dataset, all_dataloader, wfs_mean, wfs_std, norm_dict, orig_wfs, cond_vars = load_data(file_path,\n",
        "                                                                        N_FFT,\n",
        "                                                                        HOP_LENGTH,\n",
        "                                                                        WIN_LENGTH,\n",
        "                                                                        batch_size=config.train_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7PxtF975XL3"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.wfs.shape)\n",
        "print(train_dataset.cond_var.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "\n",
        "print(\"Saving to:\", config.output_dir)\n",
        "print(\"Batch size:\", config.train_batch_size)\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, val_dataset, lr_scheduler)\n",
        "\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=1, mixed_precision=config.mixed_precision)"
      ],
      "metadata": {
        "id": "-rippsYUoikA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run train.py"
      ],
      "metadata": {
        "id": "QseGaebjsM9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS5tbqcrmrJV"
      },
      "source": [
        "## MLP for max amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd_PS1ummqhN"
      },
      "outputs": [],
      "source": [
        "class MaxAmplitudePredictor(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh3p2j5gm1nQ"
      },
      "outputs": [],
      "source": [
        "wfs = train_dataset.wfs\n",
        "wfs_val = val_dataset.wfs\n",
        "\n",
        "wfs_flat = wfs.view(wfs.shape[0], -1)\n",
        "wfs_val_flat = wfs_val.view(wfs_val.shape[0], -1)\n",
        "\n",
        "train_max_amplitudes = torch.max(wfs_flat, dim=1, keepdims=True)[0]\n",
        "val_max_amplitudes = torch.max(wfs_val_flat, dim=1, keepdims=True)[0]\n",
        "\n",
        "print(train_max_amplitudes.shape)\n",
        "\n",
        "\n",
        "# Create a new dataset and dataloaders for the MLP\n",
        "class MLPDataset(Dataset):\n",
        "    def __init__(self, cond_var, max_amps):\n",
        "        self.cond_var = cond_var\n",
        "        self.max_amps = max_amps\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.cond_var.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.cond_var[idx], self.max_amps[idx]\n",
        "\n",
        "mlp_train_dataset = MLPDataset(train_dataset.cond_var, train_max_amplitudes)\n",
        "mlp_val_dataset = MLPDataset(val_dataset.cond_var, val_max_amplitudes)\n",
        "\n",
        "mlp_train_dataloader = DataLoader(mlp_train_dataset, batch_size=config.train_batch_size, shuffle=True)\n",
        "mlp_val_dataloader = DataLoader(mlp_val_dataset, batch_size=config.eval_batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAzXfTMom9xW"
      },
      "outputs": [],
      "source": [
        "input_dim = mlp_train_dataset.cond_var.shape[1]\n",
        "mlp_model = MaxAmplitudePredictor(input_dim).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop for the MLP\n",
        "num_epochs_mlp = 1000\n",
        "\n",
        "for epoch in range(num_epochs_mlp):\n",
        "    mlp_model.train()\n",
        "    running_loss = 0.0\n",
        "    for conds, max_amps in mlp_train_dataloader:\n",
        "        conds = conds.to(device).float()\n",
        "        max_amps = max_amps.to(device)\n",
        "\n",
        "        mlp_optimizer.zero_grad()\n",
        "        outputs = mlp_model(conds)\n",
        "        loss = criterion(outputs, max_amps)\n",
        "        loss.backward()\n",
        "        mlp_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * conds.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(mlp_train_dataset)\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      print(f\"MLP Epoch [{epoch+1}/{num_epochs_mlp}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Optional: Evaluate the MLP on the validation set\n",
        "    mlp_model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for conds, max_amps in mlp_val_dataloader:\n",
        "            conds = conds.to(device).float()\n",
        "            max_amps = max_amps.to(device)\n",
        "            outputs = mlp_model(conds)\n",
        "            loss = criterion(outputs, max_amps)\n",
        "            val_running_loss += loss.item() * conds.size(0)\n",
        "    val_epoch_loss = val_running_loss / len(mlp_val_dataset)\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      print(f\"MLP Validation Loss: {val_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"MLP training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugJDOrHowx2a"
      },
      "outputs": [],
      "source": [
        "example = val_dataset.cond_var[50]\n",
        "example_wf = val_dataset.wfs[50]\n",
        "\n",
        "max = example_wf.max()\n",
        "\n",
        "example = example.to(device).float()\n",
        "\n",
        "print(example)\n",
        "print(example.shape)\n",
        "preds = mlp_model(example)\n",
        "print(preds)\n",
        "print(max)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'max_amplitude.pth'\n",
        "\n",
        "torch.save(mlp_model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"MLP model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "cvU1AhsI9jtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = MaxAmplitudePredictor(input_dim)\n",
        "\n",
        "model_save_path = 'max_amplitude.pth'\n",
        "loaded_model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "loaded_model.to(device)\n",
        "\n",
        "loaded_model.eval()\n",
        "\n",
        "p = loaded_model(example)\n",
        "print(p)\n",
        "print(max)"
      ],
      "metadata": {
        "id": "zWYbJagm9yNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzzCuSdRnglM"
      },
      "source": [
        "## Results of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVwCQbPCpHKJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ynJVkCgFY_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM85okJTsSg9"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ3Pgup5s24L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# train_dataset, train_dataloader, val_dataset, val_dataloader, wfs_mean, wfs_std, norm_dict, orig_wfs = load_data(file_path,\n",
        "#                                                                                                         N_FFT,\n",
        "#                                                                                                         HOP_LENGTH,\n",
        "#                                                                                                         WIN_LENGTH,\n",
        "#                                                                                                         batch_size=1)\n",
        "\n",
        "i = random.randint(0, len(val_dataset) - 1)\n",
        "print(i)\n",
        "\n",
        "h = val_dataset.cond_var[50].to(torch.device('cuda'))\n",
        "h = h.float()\n",
        "\n",
        "max_amplitude = mlp_model(h)\n",
        "max_amplitude = max_amplitude.detach().cpu().numpy()\n",
        "\n",
        "h = cond_emb(h)\n",
        "h = h.unsqueeze(0)\n",
        "h = h.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  pipeline = CustomDDPMPipeline.from_pretrained(\"checkpoints_EW_flow_1e-4/\").to(\"cuda\")\n",
        "\n",
        "  images, samples, T = pipeline(batch_size = 1,\n",
        "                    generator = torch.manual_seed(config.seed),\n",
        "                    output_type = \"numpy\",\n",
        "                    return_dict = False,\n",
        "                    encoder_hidden_states=h)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 5,figsize=(20,10))\n",
        "\n",
        "samples = samples.cpu()\n",
        "\n",
        "# Flatten the axs array for easier indexing\n",
        "axs = axs.flatten()\n",
        "\n",
        "for i, s in enumerate(samples):\n",
        "  s = s.squeeze()\n",
        "  axs[i].imshow(s, aspect='auto')\n",
        "  axs[i].axis('off')\n",
        "  axs[i].set_title(f't= %.2f' % (T[i]))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N3zzzhA8pgzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoDVKUQvM7ut"
      },
      "outputs": [],
      "source": [
        "image = images[0]\n",
        "\n",
        "\n",
        "original = val_dataset.wfs[50].cpu().numpy()\n",
        "\n",
        "orig_max = original.max()\n",
        "\n",
        "im_1 = image[0].squeeze().cpu().numpy()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "mean = wfs_mean.cpu().numpy()\n",
        "std = wfs_std.cpu().numpy()\n",
        "\n",
        "im_1 = (im_1 * std) + mean\n",
        "original = (original * std) + mean\n",
        "\n",
        "im_1 *= max_amplitude\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"STD:\", std)\n",
        "print(\"Max amplitude:\", max_amplitude)\n",
        "print(\"Max amplitude (Original):\", orig_max)\n",
        "\n",
        "ax1.set_title(f\"Original\")\n",
        "ax1.set_xlabel('Time Frame')\n",
        "ax1.set_ylabel('Frequency Bin')\n",
        "orig = ax1.imshow(original, aspect='auto')\n",
        "fig.colorbar(orig, ax=ax1, label='Amplitude')\n",
        "\n",
        "ax2.set_title(f\"Generated\")\n",
        "ax2.set_xlabel('Time Frame')\n",
        "ax2.set_ylabel('Frequency Bin')\n",
        "gen = ax2.imshow(im_1, aspect='auto')\n",
        "fig.colorbar(gen, ax=ax2, label='Amplitude')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2HjbKqlORfY"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "wf_orig = librosa.griffinlim(original, n_iter = 512)\n",
        "wf_gen = librosa.griffinlim(im_1, n_iter = 512)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(f\"Generated waveform\")\n",
        "plt.xlabel(\"Time (miliseconds)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.ylim(wf_gen.min(), wf_gen.max())\n",
        "plt.plot(wf_gen)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(f\"Original waveform\")\n",
        "plt.xlabel(\"Time (miliseconds)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.ylim(wf_orig.min(), wf_orig.max())\n",
        "plt.plot(wf_orig)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QS5tbqcrmrJV"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOkEBgyGx/Jt5ACs/C2dvop",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}